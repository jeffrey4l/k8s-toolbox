apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llama-factory-cache
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-factory
spec:
  selector:
    matchLabels:
      app: llama-factory
  template:
    metadata:
      labels:
        app: llama-factory
    spec:
      containers:
        - name: ui
          image: llama-factory:latest
          # command: [ sleep, infinity ]
          command: [ llamafactory-cli, webui ]
          volumeMounts:
            - name: cache
              mountPath: /root/.cache
          env:
            - name: USE_MODELSCOPE_HUB
              value: "1"

          resources:
            limits:
              cpu: "4"
              memory: "8Gi"
          ports:
            - containerPort: 7860
              # nvidia.com/gpu: 1
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: llama-factory-cache
---
apiVersion: v1
kind: Service
metadata:
  name: llama-factory
spec:
  ports:
    - name: http
      port: 7860
      targetPort: 7860
  selector:
    app: llama-factory
  type: NodePort
